{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install some necessary libraries, nemreader is to read NEM files, pandas numpy are data processing libraries, and matplotlib is for creating plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nemreader numpy pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from nemreader import read_nem_file\n",
    "import datetime\n",
    "from matplotlib import pyplot\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a sample NEM12 file, read data from the General Consumption (E1) channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = read_nem_file('NEM12Sample.zip')\n",
    "suffix = 'E1'\n",
    "readings_list = []\n",
    "#for suffix in channels:\n",
    "for nmi in m.readings:\n",
    "    for channel in m.readings[nmi]:\n",
    "        for reading in m.readings[nmi][suffix]:\n",
    "            readings_list.append(reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas dataframe from the data - similar to an Excel table.\n",
    "\n",
    "(The dates in this NEM file are 2005, so I'll add ~14 years to make them 2019, in order to demonstrate 'merging' with public holiday data below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_df = pd.DataFrame(readings_list)\n",
    "readings_df['t_start'] = readings_df['t_start'].apply(lambda x : x + datetime.timedelta(days=(365.25*14)-5.5))\n",
    "readings_df['t_end'] = readings_df['t_end'].apply(lambda x : x + datetime.timedelta(days=(365.25*14)-5.5))\n",
    "readings_df['Date'] = readings_df['t_start'].apply(lambda x : x.date())\n",
    "readings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll generate a plot of the 'read_value' data just to confirm it looks as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readings_df['read_value'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Resample' the half hourly interval data to hourly data.\n",
    "\n",
    "As we're downsampling, we must specify which of the two half hourly values to retain (or whether to take the average, sum, or some other user defined function etc.).\n",
    "\n",
    "In this case we'll take the sum for 'read_value', min for start time ('t_start'), and max for end time ('t_end').\n",
    "\n",
    "Everything else is the same across both half hourly values, so we can take either the min or max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = readings_df.resample(on='t_start',rule=datetime.timedelta(hours=1))\n",
    "myfunc = lambda x : sum(x)/2\n",
    "aggfuncs = dict.fromkeys(readings_df.columns,max)\n",
    "aggfuncs.update({'read_value' : myfunc, 't_start' : min})\n",
    "hourly_df = hourly_df.agg(aggfuncs).reset_index(drop=True)\n",
    "hourly_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some other useful columns to work with. The hour of the day, and the day of the week (0=Monday, 6=Sunday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df['hour'] = hourly_df['t_start'].apply(lambda x : x.hour)\n",
    "hourly_df['weekday'] = hourly_df['t_start'].apply(lambda x : x.weekday())\n",
    "hourly_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve public holiday data from the 'data.gov.au' website, which has aggregated data for Australian public holidays.\n",
    "\n",
    "Add this in to one dataframe, then create a new dataframe (table) with just the Victorian public holidays. (Any state could be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_hols_list_of_dfs = []\n",
    "pub_hols_url_list = ['https://data.gov.au/data/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/bda4d4f2-7fde-4bfc-8a23-a6eefc8cef80/download/australian_public_holidays_2019.csv',\n",
    "                     'https://data.gov.au/data/dataset/b1bc6077-dadd-4f61-9f8c-002ab2cdff10/resource/c4163dc4-4f5a-4cae-b787-43ef0fcf8d8b/download/australian_public_holidays_2020.csv']\n",
    "for url in pub_hols_url_list:\n",
    "    pub_hols_list_of_dfs.append(pd.read_csv(url,parse_dates = ['Date']))\n",
    "pub_hols_df = pd.concat(pub_hols_list_of_dfs,sort=False,join='inner').drop(columns=['Information','More Information'])\n",
    "pub_hols_df['Date'] = pub_hols_df['Date'].apply(lambda x : x.date())\n",
    "\n",
    "state = 'vic'\n",
    "pub_hols_single_state = pub_hols_df[pub_hols_df['Jurisdiction']==state]\n",
    "pub_hols_single_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the meter data with the public holiday data. Similar to an SQL join.\n",
    "\n",
    "Create a new column for 'workdays', defined as days that are not public holidays, and where the weekday value is &lt;5 (0-4 = Monday to Friday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = hourly_df.merge(pub_hols_single_state, how='left',left_on='Date', right_on='Date')\n",
    "merged_df['workday'] = (merged_df['Holiday Name'].isna()) & (merged_df['weekday'] < 5)\n",
    "merged_df[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pivot table, showing the average values for workdays vs. non-workdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.pivot_table(index='hour',columns='workday',values='read_value',margins=True,margins_name='Total',aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hourly data for PVSELL in the required format.\n",
    "\n",
    "From: https://pvsell.zendesk.com/hc/en-us/articles/115001018408-How-do-I-upload-my-own-solar-production-data-file-\n",
    "\n",
    "A .csv file with dates in the format d/m hh:mm (e.g., 18/2 01:00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_sell_df = hourly_df[['t_start','read_value']]\n",
    "pv_sell_df['t_start'] = pv_sell_df['t_start'].apply(lambda x : x.strftime('%d/%m %H:%M'))\n",
    "pv_sell_df.to_csv('PVSelloutput.csv',header=False,index=False)\n",
    "pv_sell_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
